{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "import random\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from opencc import OpenCC\n",
    "# !pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = read_csv('3_fb_crawling_output.csv', encoding='utf8',index_col=0)\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove non-Â∫óÂÆ∂ post ###\n",
    "temp_p_lst = []\n",
    "temp_d_lst = []\n",
    "temp_p = ''\n",
    "temp_d = ''\n",
    "\n",
    "for post_content, date_time in zip(df['Description'].astype(str), df['published'].astype(str)):\n",
    "    cc = OpenCC('s2tw') # Á∞°ËΩâÁπÅ\n",
    "    post_content = cc.convert(post_content)\n",
    "    post = post_content.replace(' ','').replace('ÔøºÔøº','').replace('ÔøºÔøºÔøº','').replace('\\n','') \\\n",
    "        .replace('$','').replace('Ôºè','').replace('‚Ä¶','').replace('Ôºö',':').replace(':',':').replace('%','') \\\n",
    "        .replace(u'\\u3000',u'').replace('‚ñé','').replace('Ôºâ',')').replace('Ôºà','(').replace('O','') \\\n",
    "        .replace('„Äê',' ').replace('„Äë',':').replace('‚ñ†','').replace('‚óÜ','').replace('‚óè','').replace('‚òÖ','') \\\n",
    "        .replace('1.','').replace('2.','').replace('3.','').replace('4.','').replace('5.','') \\\n",
    "        .replace('6.','').replace('7.','') \\\n",
    "        .replace('\\'','').replace('_','').replace('‚òÜ','') \\\n",
    "        .replace('‚ñé','').replace('üÅ¢',' ').replace('-','').replace('‚óé','')\\\n",
    "        .replace('Êõ¥Â§ö','').replace('ÂàÜÈöîÁ∑ö','').replace('NT','') \\\n",
    "        .replace('‚ñéÂ∫óÂÆ∂:','Â∫óÂÆ∂:').replace('‚ñéÂ∫ó„ÄÄ„ÄÄÂêç:','Â∫óÂÆ∂:') \\\n",
    "        .replace('Â∫óÂÆ∂/ÈÑ∞Ëøë','Â∫óÂÆ∂:').replace('Â∫óÂÆ∂/ÈÑ∞ËøëÂú∞Ê®ô:','Â∫óÂÆ∂:').replace('Â∫óÂÆ∂ÈÑ∞ËøëÂú∞Èªû:','Â∫óÂÆ∂:') \\\n",
    "        .replace('Â∫óÂÆ∂/ÈÑ∞ËøëÂú∞Èªû:','Â∫óÂÆ∂:') \\\n",
    "        .replace('Â∫óÂêç:','Â∫óÂÆ∂:').replace('‚ñéÂ∫ó„ÄÄ„ÄÄÂÆ∂:','Â∫óÂÆ∂:').replace(' Â∫óÂÆ∂','Â∫óÂÆ∂:')\\\n",
    "        .replace('„ÄêÂ∫óÂÆ∂„Äë','Â∫óÂÆ∂:').replace('Â∫óÂÆ∂ÂêçÁ®±:','Â∫óÂÆ∂:').replace('Â∫óÂÆ∂Ë≥áË®ä:','Â∫óÂÆ∂:') \\\n",
    "        .replace('ÈÑ∞Ëøë:','%ÈÑ∞ËøëÂú∞Èªû:').replace('ÊâÄÂú®Âú∞ÂçÄ','%ÈÑ∞ËøëÂú∞Èªû:') \\\n",
    "        .replace('ÈÑ∞Ëøë‰ΩçÁΩÆ:','%ÈÑ∞ËøëÂú∞Èªû:').replace('Âú∞Èªû:','%ÈÑ∞ËøëÂú∞Èªû:') \\\n",
    "        .replace('ÊâÄÂú®Âú∞ÂçÄ/ÈÑ∞Ëøë‰ΩçÁΩÆ:','%ÈÑ∞ËøëÂú∞Èªû:') \\\n",
    "        .replace('ÈÑ∞ËøëÂú∞Èªû','%ÈÑ∞ËøëÂú∞Èªû').replace('Ëá®ËøëÂú∞Èªû:','%ÈÑ∞ËøëÂú∞Èªû:').replace('Âú∞ÂùÄ:','%ÈÑ∞ËøëÂú∞Èªû:') \\\n",
    "        .replace('ÈÑ∞ËøëÂú∞ÂçÄ','%ÈÑ∞ËøëÂú∞Èªû').replace('ÂçÄÂüü','%ÈÑ∞ËøëÂú∞Èªû:').replace('Âú∞ÂçÄ/ÈÑ∞Ëøë','%ÈÑ∞ËøëÂú∞Èªû:')\\\n",
    "        .replace('‰ΩçÁΩÆ:','%ÈÑ∞ËøëÂú∞Èªû:').replace('‰ΩçÁΩÆ','%ÈÑ∞ËøëÂú∞Èªû:')\\\n",
    "        .replace('ÊãâÈ∫µÂêçÁ®±','%.GÊãâÈ∫µÂêçÁ®±').replace('È§êÈªûÂêçÁ®±:','%.GÊãâÈ∫µÂêçÁ®±:')\\\n",
    "        .replace('È§êÈªû:','%.GÊãâÈ∫µÂêçÁ®±:').replace('ÊãâÈ∫µÂìÅÈ†Ö:','%.GÊãâÈ∫µÂêçÁ®±')\\\n",
    "        .replace('ÂìÅÈ†Ö:','%.GÊãâÈ∫µÂêçÁ®±:').replace('ÂìÅÂêç:','%.GÊãâÈ∫µÂêçÁ®±:')\\\n",
    "        .replace('ÂêçÁ®±:','%.GÊãâÈ∫µÂêçÁ®±:').replace('ÂìÅÈ†ÖÂÉπÊ†º:','%.GÊãâÈ∫µÂêçÁ®±:')\\\n",
    "        .replace('ÈÖçÁΩÆ:','ZÈÖçÁΩÆ').replace('ÈÖç„ÄÄ„ÄÄÁΩÆ','ZÈÖçÁΩÆ').replace('ÈÖçÁΩÆ(','ZÈÖçÁΩÆ').replace('‚ñéÈÖçÁΩÆ:','ZÈÖçÁΩÆ')\\\n",
    "        .replace('ÂøÉÂæóÊÑüÊÉ≥:','Z').replace('ÊÑüÊÉ≥:','Z').replace('ÂøÉÂæó:','Z') \\\n",
    "        .replace('ÂÆåÈ£üÊÑüÂèó:','Z').replace('ÂÆåÈ£üÊÑüÊÉ≥:','Z') \\\n",
    "        .replace('Âë≥Â¢û','Âë≥Âôå').replace('È£üÂÖ´Áï™','È£ü8Áï™').replace('‰∫î„Éé','‰∫î‰πã').replace('Á∑è','Á∏Ω')\n",
    "    \n",
    "    if \"Â∫óÂÆ∂\" in post[:5]:\n",
    "        temp_p_lst.append(post)\n",
    "\n",
    "        for word in date_time:\n",
    "            temp_d += word\n",
    "            if word == 'Êó•':\n",
    "                break\n",
    "        if ('Â∞èÊôÇ' in temp_d) or ('ÂàÜÈêò' in temp_d):\n",
    "            temp_d = '2020Âπ¥12Êúà2Êó•'\n",
    "        elif ('Êò®Â§©' in temp_d):\n",
    "            temp_d = '2020Âπ¥12Êúà1Êó•'\n",
    "        elif 'Âπ¥' not in temp_d:\n",
    "            temp_d = '2020Âπ¥' + temp_d      \n",
    "        temp_d_lst.append(temp_d)\n",
    "        temp_d = ''\n",
    "        \n",
    "#temp_p_lst\n",
    "#temp_d_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ÊúâÁÑ°Ê®ôÊ∫ñÁôºÊñáÊ†ºÂºèÂàÜÈ°û ###\n",
    "unorganized_shops = []\n",
    "unorganized_date = []\n",
    "ramen_shop_raw = []\n",
    "ramen_name_raw = []\n",
    "ramen_review_raw = []\n",
    "ramen_date_raw = []\n",
    "\n",
    "for shops, date in zip(temp_p_lst, temp_d_lst):\n",
    "    if ('%'not in shops or 'G' not in shops or 'Z' not in shops \\\n",
    "        or shops.index('Z')>shops.index('G')+80) :\n",
    "        unorganized_shops.append(shops)\n",
    "        unorganized_date.append(date)      \n",
    "    else:\n",
    "        ramen_shop_raw.append(shops[:shops.index('%')]) # ÈÑ∞ËøëÂú∞Èªû\n",
    "        ramen_name_raw.append(shops[shops.index('G')+1:shops.index('Z')])\n",
    "        ramen_review_raw.append(shops[shops.index('Z')+1:shops.index('Z')+265]+'...')\n",
    "        ramen_date_raw.append(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### second filtering\n",
    "unorganized_unorganized_shops = []\n",
    "unorganized_unorganized_date = []\n",
    "\n",
    "for shops, date in zip(unorganized_shops, unorganized_date):\n",
    "    if ('G' in shops and '0' in shops):\n",
    "        ramen_shop_raw.append(shops[:shops.index('%')])\n",
    "        ramen_name_raw.append(shops[shops.index('G')+1 : shops.index('G')+35])\n",
    "        ramen_review_raw.append(shops[shops.index('G')+15 : shops.index('G')+285]+'...')\n",
    "        ramen_date_raw.append(date)\n",
    "    else:\n",
    "        unorganized_unorganized_shops.append(shops)\n",
    "        unorganized_unorganized_date.append(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dealing with shop_name ###\n",
    "ramen_shop_list = []\n",
    "\n",
    "for shops in ramen_shop_raw:\n",
    "    shops.replace('Â∫óÂÆ∂','').replace('#','').replace('ÈÑ∞Ëøë','')\n",
    "    if ':' in shops:\n",
    "        shops = shops[shops.index(':')+1:]\n",
    "    shops = emoji.demojize(shops)\n",
    "    shops = shops.replace('Ôºè','')\n",
    "    shops = re.sub(':\\S+?:', ' ', shops)\n",
    "    if ':' in shops:\n",
    "        shops = shops[:shops.index(':')]\n",
    "    if len(shops) <= 1:\n",
    "        shops = ''\n",
    "    if 'Âú∞ÂùÄ' in shops and 'Áî®È§ê' in shops:\n",
    "        if shops.index('Âú∞') < shops.index('Áî®'):\n",
    "            ramen_shop_list.append(shops[:shops.index('Âú∞')])\n",
    "        else:\n",
    "            ramen_shop_list.append(shops[:shops.index('Áî®')])\n",
    "    elif 'Âú∞ÂùÄ' in shops:\n",
    "        ramen_shop_list.append(shops[:shops.index('Âú∞')])\n",
    "    elif 'Áî®È§ê' in shops:\n",
    "        ramen_shop_list.append(shops[:shops.index('Áî®')])\n",
    "    else:\n",
    "        ramen_shop_list.append(shops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check shop_name\n",
    "ramen_shop_list_final = []\n",
    "\n",
    "for shop in ramen_shop_list:\n",
    "    shop = re.sub(r'[^\\w\\s]','',shop)\n",
    "    shop = shop.replace('#','').replace('ÈÑ∞Ëøë','').replace('Ëá®Ëøë','').replace('ÊéíÈöäÁãÄÊ≥Å','') \\\n",
    "    .replace('Êó•Êúü','').replace('Â∫óÂÆ∂','').replace('ÈôÑËøë','').replace('„Ää','').replace('„Äã','') \\\n",
    "    .replace('„ÄÇ','').replace('„ÄÅ',' ').replace('ÔΩú','').replace('Ôºü','').replace('Âú∞ÂçÄ','') \\\n",
    "    .replace('(','').replace(')','').replace('¬∑','').replace('/','').replace('Âú∞Ê®ô','') \\\n",
    "    .replace('‚Äª','').replace('„ÅÅ','„ÅÇ').replace('¬≤','2').replace(' ','')\n",
    "    if len(shop) >= 31:\n",
    "        shop = shop[:31]\n",
    "    if 'Êç∑ÈÅã' in shop:\n",
    "        shop = shop[:shop.index('Êç∑ÈÅã')]\n",
    "    ramen_shop_list_final.append(shop)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dealing with ramen_name ###\n",
    "ramen_name_list = []\n",
    "ramen_name_list_final = []\n",
    "\n",
    "for names in ramen_name_raw:\n",
    "    names = emoji.demojize(names)\n",
    "    names = re.sub(':\\S+?:', ' ', names)\n",
    "    new_name = names.replace('ÊãâÈ∫µ%.G','').replace('%.G','').replace('#','').replace('ÈÑ∞Ëøë','')\n",
    "    last_ch = new_name[-1]\n",
    "    first_ch = new_name[0]\n",
    "    # ramen_name_list = []\n",
    "    if ('0' in new_name) and ('00' not in new_name) and ('2020' not in new_name) \\\n",
    "        and (last_ch != ')'):\n",
    "        if new_name[-1] == 'ÂÖÉ' or new_name[-2:] == 'Êó•ÂÖÉ':\n",
    "            ramen_name_list.append(new_name[:new_name.index('ÂÖÉ')+1])\n",
    "        elif new_name[-2:] == 'Êó•Âúì':\n",
    "            ramen_name_list.append(new_name[:new_name.index('Âúì')+1])\n",
    "        else:\n",
    "            ramen_name_list.append(new_name[:new_name.index('0')+1])\n",
    "    elif '00' in new_name and last_ch != ')' and '2020' not in new_name:\n",
    "        ramen_name_list.append(new_name[:new_name.index('00')+2])\n",
    "    elif first_ch != 'Êãâ' and 'Êãâ' in new_name:\n",
    "        ramen_name_list.append(new_name[new_name.index('Êãâ'):])\n",
    "    elif '0' not in new_name and '00' not in new_name and '/' not in new_name\\\n",
    "        and last_ch.isdigit() == False and last_ch != 'È∫µ' \\\n",
    "        and last_ch != ')'and '+' not in new_name:\n",
    "        new_point =  new_name.replace('È∫µ','È∫µH').replace('ÊãâÈ∫µHÂêçÁ®±','ÊãâÈ∫µÂêçÁ®±')\n",
    "        if 'H' in new_point:\n",
    "            ramen_name_list.append(new_point[:new_point.index('H')])\n",
    "        else:\n",
    "            ramen_name_list.append(new_point)\n",
    "    else:\n",
    "        ramen_name_list.append(new_name)\n",
    "\n",
    "# double check ramen_name\n",
    "for name in ramen_name_list:\n",
    "    if ':' in name:\n",
    "        name = name[name.index(':')+1:]\n",
    "    name = name.replace('ÊãâÈ∫µÂêçÁ®±/ÂÉπÊ†º','').replace('ÊãâÈ∫µÂêçÁ®±','').replace('ÊãâÈ∫µÂêçÁ®±ÂÉπÊ†º','').replace('ÂÉπÊ†º','')\n",
    "    ramen_name_list_final.append(name)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dealing with review ###\n",
    "ramen_review_list = []\n",
    "ramen_review_list_final = []\n",
    "\n",
    "for name, reviews in zip(ramen_name_list, ramen_review_raw):\n",
    "    new_reviews = reviews.replace('ÊãâÈ∫µ%.G','').replace('%.G','').replace('%','').replace('$','')\n",
    "    first_few_words = new_reviews[0:6]\n",
    "    if 'ÈÖçÁΩÆ'not in first_few_words:\n",
    "        for i in range(-(len(name)),0,1):\n",
    "            if (name[i:]) == (new_reviews[:-i]):\n",
    "                updated_review = new_reviews[-i:]\n",
    "                ramen_review_list.append(updated_review)\n",
    "                break\n",
    "            elif (i == -1) and (ramen_name_list[i:]) != (ramen_review_raw[:-i]):\n",
    "                pattern=\"[\\u4e00-\\u9fa5]+\" \n",
    "                regex = re.compile(pattern)\n",
    "                results =  regex.findall(new_reviews)\n",
    "                results_to_str =' '.join([str(elem) for elem in results]) \n",
    "                ramen_review_list.append(results_to_str)\n",
    "    else:\n",
    "        ramen_review_list.append(new_reviews)\n",
    "\n",
    "# double check\n",
    "punc = [',' , '.' , 'Ôºå', '„ÄÇ', ')',' ','‚Ä¶‚Ä¶','Ôºõ','„ÄÅ','\\'','/','?','ÂÖÉ']\n",
    "for reviews in ramen_review_list:\n",
    "    if reviews[0] in punc:\n",
    "        reviews = reviews[1:]\n",
    "    ramen_review_list_final.append(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create post_id \n",
    "post_id = [i for i in range(len(ramen_shop_list))]\n",
    "random.shuffle(post_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create stem_store_name\n",
    "stem_store = []\n",
    "\n",
    "for store in ramen_shop_list_final:\n",
    "    store = store.lower()\n",
    "    store = re.sub(r'[^\\w\\s]','',store)\n",
    "    store = store.replace('Á∑è','Á∏Ω').replace('È∫∫','È∫µ').replace('ÈÜ§','ÈÜ¨') \\\n",
    "                .replace('È∑πÊµÅÊù±‰∫¨Ë±öÈ™®ÊãâÈ∫µÊ•µÂå†','È∑πÊµÅÊ•µÂå†').replace('È∑πÊµÅÊù±‰∫¨ÈÜ¨Ê≤πÊãâÈ∫µËò≠‰∏∏','È∑πÊµÅËò≠‰∏∏').replace('È∑πÊµÅËá∫ÁÅ£Êú¨Â∫ó','È∑πÊµÅÊãâÈ∫µËá∫ÁÅ£Êú¨Â∫ó') \\\n",
    "                .replace('„É©„Éº„É°„É≥','ÊãâÈ∫µ').replace('„Çâ„Éº„ÇÅ„Çì','ÊãâÈ∫µ').replace('ÊüëÊ©òshin','ÊüëÊ©òshinn').replace('È∫µÂ±ãÂ£π‰πãÁ©¥','È∫µÂ±ãÂ£π‰πãÁ©¥ichi') \\\n",
    "                .replace('„ÅÆ','‰πã').replace('aqua2','').replace('È∫µÈã™','È∫µËàñ').replace('Âè∞Êπæ','Ëá∫ÁÅ£').replace('Áï™ËåÑ','ËïÉËåÑ').replace('Âè∞','Ëá∫')  \n",
    "    stem_store.append(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Output_csv\n",
    "df_output = pd.DataFrame(list(zip(*[post_id, ramen_shop_list_final, stem_store, temp_d_lst, ramen_name_list_final, ramen_review_list_final])))\n",
    "col_names = ['post_id', 'stores', 'stem_store', 'create_on', 'ramen_name', 'fb_review']\n",
    "df_output.columns = col_names\n",
    "df_output.to_csv('2_output_fb_crawling.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping store_name (for fb testing only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, itertools\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = read_csv('2_output_fb_crawling.csv', encoding='utf8',index_col=0)\n",
    "#df_output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuchiacheng/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"\"\"\n",
      "/Users/yuchiacheng/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df_output[\"stores_len\"] = df_output[\"stores\"].str.len()\n",
    "df_output[\"ramen_name_len\"] = df_output[\"ramen_name\"].str.len() \n",
    "df_output[\"fb_review_len\"] = df_output[\"fb_review\"].str.len() \n",
    "df_output_ = df_output[(df_output['stores_len'] > 1.0)]\n",
    "df_output_ = df_output_[(df_output['ramen_name_len'] > 1.0)]\n",
    "df_output_ = df_output_[(df_output['fb_review_len'] > 2.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_output_.sort_values(by=['stores', 'ramen_name_len'])\n",
    "df_new = df_new.reset_index(drop=True)\n",
    "df_new = df_new.drop(columns=['stores_len','ramen_name_len','fb_review_len'])\n",
    "df_new = df_new.sort_index(axis=0 ,ascending=True)\n",
    "df_new = df_new.iloc[::-1]\n",
    "df_new = df_new.reset_index(drop=True)\n",
    "#df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_csv('final_fb_crawling.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "567"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores_input = list(df_new['stem_store'])\n",
    "shop_sorted = sorted(list(set(stores_input)))\n",
    "len(shop_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "util_func = lambda x: x[0]  \n",
    "first_sort = [list(ele) for i, ele in groupby(shop_sorted, util_func)] \n",
    "lst = list(itertools.chain(*first_sort))\n",
    "#first_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = []\n",
    "second_sort = []\n",
    "\n",
    "util_func = lambda x: x[1]\n",
    "for item in first_sort:\n",
    "    if len(item) > 1:\n",
    "        temp = sorted(item, key = util_func)\n",
    "        second_sort.append([list(ele) for i, ele in groupby(temp, util_func)])\n",
    "        #second_sort.append([list(g) for _, g in itertools.groupby(item, lambda x: x[1])])\n",
    "    else:\n",
    "        unique.append(item[0])\n",
    "\n",
    "second_sort_len = []\n",
    "\n",
    "for item_1 in second_sort:\n",
    "    for item_2 in item_1: \n",
    "        if len(item_2) > 1:\n",
    "            item_2 = sorted(item_2, key=len)\n",
    "            second_sort_len.append(item_2)\n",
    "        else:\n",
    "            unique.append(item_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "lst = list(itertools.chain(*second_sort_len))\n",
    "#len(lst) # 362\n",
    "#len(unique) # 177"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_sort = []\n",
    "\n",
    "util_func = lambda x: x[2]\n",
    "for item in second_sort_len:\n",
    "    if item[0][:2] == 'Â±±Âµê' or item[0][:3] == 'Ëµ§È∫µÂª†' or item[0][:4] == 'È∫µÂÆ∂‰∏âÂ£´' \\\n",
    "        or item[0][:4] == 'ÊãâÈ∫µ‰∫åÈÉé' or item[0][:3] == '‰∏ÄÈ¢®Â†Ç' or item[0][:4] == 'È≥•‰∫∫ÊãâÈ∫µ' or item[0][:4] == 'Â§™ÈôΩ' \\\n",
    "        or item[0][:4] == 'Á•ûÂ±±ÊãâÈ∫µ'or item[0][:4] == 'Ëµ§ÂùÇÊãâÈ∫µ' or item[0][:4] == 'ÂäõÈáèÊãâÈ∫µ' or item[0][:4]== '‰∫¨Ê≠£ÊãâÈ∫µ':\n",
    "        third_sort.append([item])   \n",
    "    else:\n",
    "        if len(item) > 1 and len(item[0]) >= 3:\n",
    "            temp = sorted(item, key = util_func)\n",
    "            third_sort.append([list(ele) for i, ele in groupby(temp, util_func)])\n",
    "            #third_sort.append([list(g) for _, g in itertools.groupby(item, lambda x: x[2])])\n",
    "        elif len(item) > 1 and len(item[0]) == 2:\n",
    "            third_sort.append([item])\n",
    "        else:\n",
    "            unique.append(item[0])\n",
    "        \n",
    "third_sort_len = []\n",
    "\n",
    "for item_1 in third_sort:\n",
    "    for item_2 in item_1: \n",
    "        if len(item_2) > 1:\n",
    "            item_2 = sorted(item_2, key=len)\n",
    "            third_sort_len.append(item_2)\n",
    "        else:\n",
    "            unique.append(item_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "lst = list(itertools.chain(*third_sort_len))\n",
    "#print(len(lst)) # 362\n",
    "#len(unique) #254"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourth_sort = []\n",
    "\n",
    "util_func = lambda x: x[3]\n",
    "for item in third_sort_len:\n",
    "    if item[0][:2] == 'Â±±Âµê' or item[0][:3] == 'Ëµ§È∫µÂª†' or item[0][:4] == 'È∫µÂÆ∂‰∏âÂ£´' \\\n",
    "        or item[0][:4] == 'ÊãâÈ∫µ‰∫åÈÉé' or item[0][:3] == '‰∏ÄÈ¢®Â†Ç' or item[0][:4] == 'È≥•‰∫∫ÊãâÈ∫µ' or item[0][:4] == 'Â§™ÈôΩ' \\\n",
    "        or item[0][:4] == 'Á•ûÂ±±ÊãâÈ∫µ'or item[0][:4] == 'Ëµ§ÂùÇÊãâÈ∫µ' or item[0][:4] == 'ÂäõÈáèÊãâÈ∫µ' or item[0][:4]== '‰∫¨Ê≠£ÊãâÈ∫µ':\n",
    "        fourth_sort.append([item])   \n",
    "    else:\n",
    "        if len(item) > 1 and len(item[0]) >= 4:\n",
    "            temp = sorted(item, key = util_func)\n",
    "            fourth_sort.append([list(ele) for i, ele in groupby(temp, util_func)])        \n",
    "        elif len(item) > 1 and len(item[0]) <= 3:\n",
    "            fourth_sort.append([item])\n",
    "        else:\n",
    "            unique.append(item[0])\n",
    "\n",
    "fourth_sort_len = []\n",
    "\n",
    "for item_1 in fourth_sort:\n",
    "    for item_2 in item_1: \n",
    "        if len(item_2) > 1:\n",
    "            item_2 = sorted(item_2, key=len)\n",
    "            fourth_sort_len.append(item_2)\n",
    "        else:\n",
    "            unique.append(item_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "lst = list(itertools.chain(*fourth_sort_len))\n",
    "#print(len(lst))\n",
    "#len(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = []\n",
    "unclassified_group = []\n",
    "\n",
    "for item in fourth_sort_len:\n",
    "    grouped = []\n",
    "    unclassified = []\n",
    "    grouped.append(item[0])\n",
    "    for i in range(len(item)-1):\n",
    "        if (item[0] in item[i+1]) or (item[0][:6] in item[i+1]):\n",
    "            grouped.append(item[i+1])\n",
    "        else:\n",
    "            unclassified.append(item[i+1])\n",
    "    done.append(grouped)\n",
    "    if unclassified != []:\n",
    "        unclassified_group.append(unclassified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "lst = list(itertools.chain(*done))\n",
    "#print(len(lst))\n",
    "#print(len(unique))\n",
    "clst = list(itertools.chain(*unclassified_group))\n",
    "#print(len(clst))\n",
    "#unclassified_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in unclassified_group:\n",
    "    if len(item) != 1:\n",
    "        temp_grouped = []\n",
    "        temp_unclassified = []\n",
    "        temp_grouped.append(item[0])\n",
    "        temp_unclassified.append(item[0])\n",
    "        for i in range(len(item)-1):\n",
    "            if (item[0] in item[i+1]) or (item[0][:len(item[0])//2] in item[i+1]):\n",
    "                temp_grouped.append(item[i+1])\n",
    "            else:\n",
    "                temp_unclassified.append(item[i+1])\n",
    "        done.append(temp_grouped)\n",
    "        if len(temp_unclassified) >= 2:\n",
    "            print(temp_unclassified)\n",
    "            unique.append(temp_unclassified[:])\n",
    "    else:\n",
    "        unique.append(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = list(itertools.chain(*done))\n",
    "#print(len(lst))\n",
    "#print(len(unique))\n",
    "#unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_done = {}\n",
    "for i in range(len(done)):\n",
    "    dict_done[i] = done[i]\n",
    "#dict_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_id = []\n",
    "\n",
    "for store in list(df_new['stem_store']):\n",
    "    for key, v in dict_done.items():\n",
    "        if store in v:\n",
    "            store_id.append(key)\n",
    "            break\n",
    "        elif store in unique:\n",
    "            store_id.append(unique.index(store)+500)\n",
    "            break\n",
    "        elif (store not in unique) and (key == list(dict_done.keys())[-1]):\n",
    "            store_id.append('999')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_new['store_id'] = store_id\n",
    "df_new.to_csv('store_id_done.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python37364bitbasecondaa34aaf43f98b497db7a5252c786d5680"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
