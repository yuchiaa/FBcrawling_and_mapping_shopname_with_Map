{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "import random\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from opencc import OpenCC\n",
    "# !pip install emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organizing Facebook Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load data from Fb_crawling_output\n",
    "df = read_csv('3_fb_crawling_output.csv', encoding='utf8',index_col=0)\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove non-\"Â∫óÂÆ∂\" post ###\n",
    "temp_p_lst = []\n",
    "temp_d_lst = []\n",
    "temp_p = ''\n",
    "temp_d = ''\n",
    "\n",
    "for post_content, date_time in zip(df['Description'].astype(str), df['published'].astype(str)):\n",
    "    cc = OpenCC('s2tw') # Á∞°ËΩâÁπÅ\n",
    "    post_content = cc.convert(post_content)\n",
    "    post = post_content.replace(' ','').replace('ÔøºÔøº','').replace('ÔøºÔøºÔøº','').replace('\\n','') \\\n",
    "        .replace('$','').replace('Ôºè','').replace('‚Ä¶','').replace('Ôºö',':').replace(':',':').replace('%','') \\\n",
    "        .replace(u'\\u3000',u'').replace('‚ñé','').replace('Ôºâ',')').replace('Ôºà','(').replace('O','') \\\n",
    "        .replace('„Äê',' ').replace('„Äë',':').replace('‚ñ†','').replace('‚óÜ','').replace('‚óè','').replace('‚òÖ','') \\\n",
    "        .replace('1.','').replace('2.','').replace('3.','').replace('4.','').replace('5.','') \\\n",
    "        .replace('6.','').replace('7.','') \\\n",
    "        .replace('\\'','').replace('_','').replace('‚òÜ','') \\\n",
    "        .replace('‚ñé','').replace('üÅ¢',' ').replace('-','').replace('‚óé','')\\\n",
    "        .replace('Êõ¥Â§ö','').replace('ÂàÜÈöîÁ∑ö','').replace('NT','') \\\n",
    "        .replace('‚ñéÂ∫óÂÆ∂:','Â∫óÂÆ∂:').replace('‚ñéÂ∫ó„ÄÄ„ÄÄÂêç:','Â∫óÂÆ∂:') \\\n",
    "        .replace('Â∫óÂÆ∂/ÈÑ∞Ëøë','Â∫óÂÆ∂:').replace('Â∫óÂÆ∂/ÈÑ∞ËøëÂú∞Ê®ô:','Â∫óÂÆ∂:').replace('Â∫óÂÆ∂ÈÑ∞ËøëÂú∞Èªû:','Â∫óÂÆ∂:') \\\n",
    "        .replace('Â∫óÂÆ∂/ÈÑ∞ËøëÂú∞Èªû:','Â∫óÂÆ∂:') \\\n",
    "        .replace('Â∫óÂêç:','Â∫óÂÆ∂:').replace('‚ñéÂ∫ó„ÄÄ„ÄÄÂÆ∂:','Â∫óÂÆ∂:').replace(' Â∫óÂÆ∂','Â∫óÂÆ∂:')\\\n",
    "        .replace('„ÄêÂ∫óÂÆ∂„Äë','Â∫óÂÆ∂:').replace('Â∫óÂÆ∂ÂêçÁ®±:','Â∫óÂÆ∂:').replace('Â∫óÂÆ∂Ë≥áË®ä:','Â∫óÂÆ∂:') \\\n",
    "        .replace('ÈÑ∞Ëøë:','%ÈÑ∞ËøëÂú∞Èªû:').replace('ÊâÄÂú®Âú∞ÂçÄ','%ÈÑ∞ËøëÂú∞Èªû:') \\\n",
    "        .replace('ÈÑ∞Ëøë‰ΩçÁΩÆ:','%ÈÑ∞ËøëÂú∞Èªû:').replace('Âú∞Èªû:','%ÈÑ∞ËøëÂú∞Èªû:') \\\n",
    "        .replace('ÊâÄÂú®Âú∞ÂçÄ/ÈÑ∞Ëøë‰ΩçÁΩÆ:','%ÈÑ∞ËøëÂú∞Èªû:') \\\n",
    "        .replace('ÈÑ∞ËøëÂú∞Èªû','%ÈÑ∞ËøëÂú∞Èªû').replace('Ëá®ËøëÂú∞Èªû:','%ÈÑ∞ËøëÂú∞Èªû:').replace('Âú∞ÂùÄ:','%ÈÑ∞ËøëÂú∞Èªû:') \\\n",
    "        .replace('ÈÑ∞ËøëÂú∞ÂçÄ','%ÈÑ∞ËøëÂú∞Èªû').replace('ÂçÄÂüü','%ÈÑ∞ËøëÂú∞Èªû:').replace('Âú∞ÂçÄ/ÈÑ∞Ëøë','%ÈÑ∞ËøëÂú∞Èªû:')\\\n",
    "        .replace('‰ΩçÁΩÆ:','%ÈÑ∞ËøëÂú∞Èªû:').replace('‰ΩçÁΩÆ','%ÈÑ∞ËøëÂú∞Èªû:')\\\n",
    "        .replace('ÊãâÈ∫µÂêçÁ®±','%.GÊãâÈ∫µÂêçÁ®±').replace('È§êÈªûÂêçÁ®±:','%.GÊãâÈ∫µÂêçÁ®±:')\\\n",
    "        .replace('È§êÈªû:','%.GÊãâÈ∫µÂêçÁ®±:').replace('ÊãâÈ∫µÂìÅÈ†Ö:','%.GÊãâÈ∫µÂêçÁ®±')\\\n",
    "        .replace('ÂìÅÈ†Ö:','%.GÊãâÈ∫µÂêçÁ®±:').replace('ÂìÅÂêç:','%.GÊãâÈ∫µÂêçÁ®±:')\\\n",
    "        .replace('ÂêçÁ®±:','%.GÊãâÈ∫µÂêçÁ®±:').replace('ÂìÅÈ†ÖÂÉπÊ†º:','%.GÊãâÈ∫µÂêçÁ®±:')\\\n",
    "        .replace('ÈÖçÁΩÆ:','ZÈÖçÁΩÆ').replace('ÈÖç„ÄÄ„ÄÄÁΩÆ','ZÈÖçÁΩÆ').replace('ÈÖçÁΩÆ(','ZÈÖçÁΩÆ').replace('‚ñéÈÖçÁΩÆ:','ZÈÖçÁΩÆ')\\\n",
    "        .replace('ÂøÉÂæóÊÑüÊÉ≥:','Z').replace('ÊÑüÊÉ≥:','Z').replace('ÂøÉÂæó:','Z') \\\n",
    "        .replace('ÂÆåÈ£üÊÑüÂèó:','Z').replace('ÂÆåÈ£üÊÑüÊÉ≥:','Z') \\\n",
    "        .replace('Âë≥Â¢û','Âë≥Âôå').replace('È£üÂÖ´Áï™','È£ü8Áï™').replace('‰∫î„Éé','‰∫î‰πã').replace('Á∑è','Á∏Ω')\n",
    "    \n",
    "    if \"Â∫óÂÆ∂\" in post[:5]:\n",
    "        temp_p_lst.append(post)\n",
    "\n",
    "        for word in date_time:\n",
    "            temp_d += word\n",
    "            if word == 'Êó•':\n",
    "                break\n",
    "        if ('Â∞èÊôÇ' in temp_d) or ('ÂàÜÈêò' in temp_d):\n",
    "            temp_d = '2020Âπ¥12Êúà2Êó•'   # Change the current date as nedded\n",
    "        elif ('Êò®Â§©' in temp_d):\n",
    "            temp_d = '2020Âπ¥12Êúà1Êó•'   # Change the date as nedded\n",
    "        elif 'Âπ¥' not in temp_d:\n",
    "            temp_d = '2020Âπ¥' + temp_d      \n",
    "        temp_d_lst.append(temp_d)\n",
    "        temp_d = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ÊúâÁÑ°Ê®ôÊ∫ñÁôºÊñáÊ†ºÂºèÂàÜÈ°û \n",
    "unorganized_shops = []\n",
    "unorganized_date = []\n",
    "ramen_shop_raw = []\n",
    "ramen_name_raw = []\n",
    "ramen_review_raw = []\n",
    "ramen_date_raw = []\n",
    "\n",
    "for shops, date in zip(temp_p_lst, temp_d_lst):\n",
    "    if ('%'not in shops or 'G' not in shops or 'Z' not in shops \\\n",
    "        or shops.index('Z')>shops.index('G')+80) :\n",
    "        unorganized_shops.append(shops)\n",
    "        unorganized_date.append(date)      \n",
    "    else:\n",
    "        ramen_shop_raw.append(shops[:shops.index('%')]) # ÈÑ∞ËøëÂú∞Èªû\n",
    "        ramen_name_raw.append(shops[shops.index('G')+1:shops.index('Z')])\n",
    "        ramen_review_raw.append(shops[shops.index('Z')+1:shops.index('Z')+265]+'...')\n",
    "        ramen_date_raw.append(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### second filtering\n",
    "unorganized_unorganized_shops = []\n",
    "unorganized_unorganized_date = []\n",
    "\n",
    "for shops, date in zip(unorganized_shops, unorganized_date):\n",
    "    if ('G' in shops and '0' in shops):\n",
    "        ramen_shop_raw.append(shops[:shops.index('%')])\n",
    "        ramen_name_raw.append(shops[shops.index('G')+1 : shops.index('G')+35])\n",
    "        ramen_review_raw.append(shops[shops.index('G')+15 : shops.index('G')+285]+'...')\n",
    "        ramen_date_raw.append(date)\n",
    "    else:\n",
    "        unorganized_unorganized_shops.append(shops)\n",
    "        unorganized_unorganized_date.append(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dealing with shop_name\n",
    "ramen_shop_list = []\n",
    "\n",
    "for shops in ramen_shop_raw:\n",
    "    shops.replace('Â∫óÂÆ∂','').replace('#','').replace('ÈÑ∞Ëøë','')\n",
    "    if ':' in shops:\n",
    "        shops = shops[shops.index(':')+1:]\n",
    "    shops = emoji.demojize(shops)\n",
    "    shops = shops.replace('Ôºè','')\n",
    "    shops = re.sub(':\\S+?:', ' ', shops)\n",
    "    if ':' in shops:\n",
    "        shops = shops[:shops.index(':')]\n",
    "    if len(shops) <= 1:\n",
    "        shops = ''\n",
    "    if 'Âú∞ÂùÄ' in shops and 'Áî®È§ê' in shops:\n",
    "        if shops.index('Âú∞') < shops.index('Áî®'):\n",
    "            ramen_shop_list.append(shops[:shops.index('Âú∞')])\n",
    "        else:\n",
    "            ramen_shop_list.append(shops[:shops.index('Áî®')])\n",
    "    elif 'Âú∞ÂùÄ' in shops:\n",
    "        ramen_shop_list.append(shops[:shops.index('Âú∞')])\n",
    "    elif 'Áî®È§ê' in shops:\n",
    "        ramen_shop_list.append(shops[:shops.index('Áî®')])\n",
    "    else:\n",
    "        ramen_shop_list.append(shops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### double check shop_name\n",
    "ramen_shop_list_final = []\n",
    "\n",
    "for shop in ramen_shop_list:\n",
    "    shop = re.sub(r'[^\\w\\s]','',shop)\n",
    "    shop = shop.replace('#','').replace('ÈÑ∞Ëøë','').replace('Ëá®Ëøë','').replace('ÊéíÈöäÁãÄÊ≥Å','') \\\n",
    "    .replace('Êó•Êúü','').replace('Â∫óÂÆ∂','').replace('ÈôÑËøë','').replace('„Ää','').replace('„Äã','') \\\n",
    "    .replace('„ÄÇ','').replace('„ÄÅ',' ').replace('ÔΩú','').replace('Ôºü','').replace('Âú∞ÂçÄ','') \\\n",
    "    .replace('(','').replace(')','').replace('¬∑','').replace('/','').replace('Âú∞Ê®ô','') \\\n",
    "    .replace('‚Äª','').replace('„ÅÅ','„ÅÇ').replace('¬≤','2').replace(' ','')\n",
    "    if len(shop) >= 31:\n",
    "        shop = shop[:31]\n",
    "    if 'Êç∑ÈÅã' in shop:\n",
    "        shop = shop[:shop.index('Êç∑ÈÅã')]\n",
    "    ramen_shop_list_final.append(shop)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dealing with ramen_name \n",
    "ramen_name_list = []\n",
    "ramen_name_list_final = []\n",
    "\n",
    "for names in ramen_name_raw:\n",
    "    names = emoji.demojize(names)\n",
    "    names = re.sub(':\\S+?:', ' ', names)\n",
    "    new_name = names.replace('ÊãâÈ∫µ%.G','').replace('%.G','').replace('#','').replace('ÈÑ∞Ëøë','')\n",
    "    last_ch = new_name[-1]\n",
    "    first_ch = new_name[0]\n",
    "    # ramen_name_list = []\n",
    "    if ('0' in new_name) and ('00' not in new_name) and ('2020' not in new_name) \\\n",
    "        and (last_ch != ')'):\n",
    "        if new_name[-1] == 'ÂÖÉ' or new_name[-2:] == 'Êó•ÂÖÉ':\n",
    "            ramen_name_list.append(new_name[:new_name.index('ÂÖÉ')+1])\n",
    "        elif new_name[-2:] == 'Êó•Âúì':\n",
    "            ramen_name_list.append(new_name[:new_name.index('Âúì')+1])\n",
    "        else:\n",
    "            ramen_name_list.append(new_name[:new_name.index('0')+1])\n",
    "    elif '00' in new_name and last_ch != ')' and '2020' not in new_name:\n",
    "        ramen_name_list.append(new_name[:new_name.index('00')+2])\n",
    "    elif first_ch != 'Êãâ' and 'Êãâ' in new_name:\n",
    "        ramen_name_list.append(new_name[new_name.index('Êãâ'):])\n",
    "    elif '0' not in new_name and '00' not in new_name and '/' not in new_name\\\n",
    "        and last_ch.isdigit() == False and last_ch != 'È∫µ' \\\n",
    "        and last_ch != ')'and '+' not in new_name:\n",
    "        new_point =  new_name.replace('È∫µ','È∫µH').replace('ÊãâÈ∫µHÂêçÁ®±','ÊãâÈ∫µÂêçÁ®±')\n",
    "        if 'H' in new_point:\n",
    "            ramen_name_list.append(new_point[:new_point.index('H')])\n",
    "        else:\n",
    "            ramen_name_list.append(new_point)\n",
    "    else:\n",
    "        ramen_name_list.append(new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### double check ramen_name\n",
    "for name in ramen_name_list:\n",
    "    if ':' in name:\n",
    "        name = name[name.index(':')+1:]\n",
    "    name = name.replace('ÊãâÈ∫µÂêçÁ®±/ÂÉπÊ†º','').replace('ÊãâÈ∫µÂêçÁ®±','').replace('ÊãâÈ∫µÂêçÁ®±ÂÉπÊ†º','').replace('ÂÉπÊ†º','')\n",
    "    ramen_name_list_final.append(name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dealing with review\n",
    "ramen_review_list = []\n",
    "ramen_review_list_final = []\n",
    "\n",
    "for name, reviews in zip(ramen_name_list, ramen_review_raw):\n",
    "    new_reviews = reviews.replace('ÊãâÈ∫µ%.G','').replace('%.G','').replace('%','').replace('$','')\n",
    "    first_few_words = new_reviews[0:6]\n",
    "    if 'ÈÖçÁΩÆ'not in first_few_words:\n",
    "        for i in range(-(len(name)),0,1):\n",
    "            if (name[i:]) == (new_reviews[:-i]):\n",
    "                updated_review = new_reviews[-i:]\n",
    "                ramen_review_list.append(updated_review)\n",
    "                break\n",
    "            elif (i == -1) and (ramen_name_list[i:]) != (ramen_review_raw[:-i]):\n",
    "                pattern=\"[\\u4e00-\\u9fa5]+\" \n",
    "                regex = re.compile(pattern)\n",
    "                results =  regex.findall(new_reviews)\n",
    "                results_to_str =' '.join([str(elem) for elem in results]) \n",
    "                ramen_review_list.append(results_to_str)\n",
    "    else:\n",
    "        ramen_review_list.append(new_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### double check review\n",
    "punc = [',' , '.' , 'Ôºå', '„ÄÇ', ')',' ','‚Ä¶‚Ä¶','Ôºõ','„ÄÅ','\\'','/','?','ÂÖÉ']\n",
    "for reviews in ramen_review_list:\n",
    "    if reviews[0] in punc:\n",
    "        reviews = reviews[1:]\n",
    "    ramen_review_list_final.append(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'post_id = [i for i in range(len(ramen_shop_list))]\\nrandom.shuffle(post_id)'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create post_id \n",
    "post_id = ['t3'+ str(i) for i in range(len(ramen_shop_list))]\n",
    "random.shuffle(post_id)\n",
    "'''post_id = [i for i in range(len(ramen_shop_list))]\n",
    "random.shuffle(post_id)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create stem_store_name\n",
    "stem_store = []\n",
    "\n",
    "for store in ramen_shop_list_final:\n",
    "    store = store.lower()\n",
    "    store = re.sub(r'[^\\w\\s]','',store)\n",
    "    store = store.replace('Á∑è','Á∏Ω').replace('È∫∫','È∫µ').replace('ÈÜ§','ÈÜ¨').replace('„Çâ„Éº„ÇÅ„Çì‰∏ÉÈù¢','‰∏ÉÈù¢') \\\n",
    "                .replace('Ëá∫ÂçóÁôΩÈú≤','ÁôΩÈú≤').replace('È∑πÊµÅÊù±‰∫¨Ë±öÈ™®ÊãâÈ∫µÊ•µÂå†','È∑πÊµÅÊ•µÂå†') \\\n",
    "                .replace('Â§™ÈôΩËïÉËåÑEXPRESS','Â§™ÈôΩËïÉËåÑÊãâÈ∫µEXPRESS') \\\n",
    "                .replace('È∑πÊµÅÊù±‰∫¨ÈÜ¨Ê≤πÊãâÈ∫µËò≠‰∏∏','È∑πÊµÅËò≠‰∏∏').replace('È∑πÊµÅËá∫ÁÅ£Êú¨Â∫ó','È∑πÊµÅÊãâÈ∫µËá∫ÁÅ£Êú¨Â∫ó') \\\n",
    "                .replace('„É©„Éº„É°„É≥','ÊãâÈ∫µ').replace('„Çâ„Éº„ÇÅ„Çì','ÊãâÈ∫µ').replace('ÊüëÊ©òshin','ÊüëÊ©òshinn')  \\\n",
    "                .replace('È∫µÂ±ãÂ£π‰πãÁ©¥','È∫µÂ±ãÂ£π‰πãÁ©¥ichi').replace('„ÅÆ','‰πã').replace('aqua2','') \\\n",
    "                .replace('È∫µÈã™','È∫µËàñ').replace('Âè∞Êπæ','Ëá∫ÁÅ£').replace('Áï™ËåÑ','ËïÉËåÑ')  \\\n",
    "                .replace('Âè∞','Ëá∫').replace('È∫µ„ÇÑÈùíÈà¥','È∫µÂ±ãÈùíÈà¥')\n",
    "    stem_store.append(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuchiacheng/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  if __name__ == '__main__':\n",
      "/Users/yuchiacheng/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "#### Output_csv\n",
    "df_output = pd.DataFrame(list(zip(*[post_id, ramen_shop_list_final, stem_store, ramen_date_raw, ramen_name_list_final, ramen_review_list_final])))\n",
    "col_names = ['post_id', 'stores', 'stem_store', 'create_on', 'ramen_name', 'fb_review']\n",
    "df_output.columns = col_names\n",
    "df_output[\"stores_len\"] = df_output[\"stores\"].str.len()\n",
    "df_output[\"ramen_name_len\"] = df_output[\"ramen_name\"].str.len() \n",
    "df_output[\"fb_review_len\"] = df_output[\"fb_review\"].str.len() \n",
    "df_output_ = df_output[(df_output['stores_len'] > 1.0)]\n",
    "df_output_ = df_output_[(df_output['ramen_name_len'] > 1.0)]\n",
    "df_output_ = df_output_[(df_output['fb_review_len'] > 2.0)]\n",
    "\n",
    "df_new = df_output_.sort_values(by=['stores', 'ramen_name_len'])\n",
    "df_new = df_new.reset_index(drop=True)\n",
    "df_new = df_new.drop(columns=['stores_len','ramen_name_len','fb_review_len'])\n",
    "df_new = df_new.sort_index(axis=0 ,ascending=True)\n",
    "df_new = df_new.iloc[::-1]\n",
    "df_new = df_new.reset_index(drop=True)\n",
    "df_new.to_csv('date_2_output_fb_crawling.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping store_name with Map (create store_id) -> TABLE3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, itertools\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = read_csv('2_output_fb_crawling.csv', encoding='utf8',index_col=0)\n",
    "stores_input = list(df_new['stem_store'])\n",
    "shop_sorted = sorted(list(set(stores_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {},
   "outputs": [],
   "source": [
    "util_func = lambda x: x[0]  \n",
    "first_sort = [list(ele) for i, ele in groupby(shop_sorted, util_func)] \n",
    "lst = list(itertools.chain(*first_sort))\n",
    "#first_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SECOND GROUPING\n",
    "unique = []\n",
    "second_sort = []\n",
    "\n",
    "util_func = lambda x: x[1]\n",
    "for item in first_sort:\n",
    "    if len(item) > 1:\n",
    "        temp = sorted(item, key = util_func)\n",
    "        second_sort.append([list(ele) for i, ele in groupby(temp, util_func)])\n",
    "    else:\n",
    "        unique.append(item[0])\n",
    "\n",
    "second_sort_len = []\n",
    "\n",
    "for item_1 in second_sort:\n",
    "    for item_2 in item_1: \n",
    "        if len(item_2) > 1:\n",
    "            item_2 = sorted(item_2, key=len)\n",
    "            second_sort_len.append(item_2)\n",
    "        else:\n",
    "            unique.append(item_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIRD GROUPING\n",
    "third_sort = []\n",
    "\n",
    "util_func = lambda x: x[2]\n",
    "for item in second_sort_len:\n",
    "    if item[0][:2] == 'Â±±Âµê' or item[0][:3] == 'Ëµ§È∫µÂª†' or item[0][:4] == 'È∫µÂÆ∂‰∏âÂ£´' \\\n",
    "        or item[0][:4] == 'ÊãâÈ∫µ‰∫åÈÉé' or item[0][:3] == '‰∏ÄÈ¢®Â†Ç' or item[0][:4] == 'È≥•‰∫∫ÊãâÈ∫µ' or item[0][:4] == 'Â§™ÈôΩ' \\\n",
    "        or item[0][:4] == 'Á•ûÂ±±ÊãâÈ∫µ'or item[0][:4] == 'Ëµ§ÂùÇÊãâÈ∫µ' or item[0][:4] == 'ÂäõÈáèÊãâÈ∫µ' or item[0][:4]== '‰∫¨Ê≠£ÊãâÈ∫µ':\n",
    "        third_sort.append([item])   \n",
    "    else:\n",
    "        if len(item) > 1 and len(item[0]) >= 3:\n",
    "            temp = sorted(item, key = util_func)\n",
    "            third_sort.append([list(ele) for i, ele in groupby(temp, util_func)])\n",
    "        elif len(item) > 1 and len(item[0]) == 2:\n",
    "            third_sort.append([item])\n",
    "        else:\n",
    "            unique.append(item[0])\n",
    "        \n",
    "third_sort_len = []\n",
    "\n",
    "for item_1 in third_sort:\n",
    "    for item_2 in item_1: \n",
    "        if len(item_2) > 1:\n",
    "            item_2 = sorted(item_2, key=len)\n",
    "            third_sort_len.append(item_2)\n",
    "        else:\n",
    "            unique.append(item_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOURTH GROUPING\n",
    "fourth_sort = []\n",
    "\n",
    "util_func = lambda x: x[3]\n",
    "for item in third_sort_len:\n",
    "    if item[0][:2] == 'Â±±Âµê' or item[0][:3] == 'Ëµ§È∫µÂª†' or item[0][:4] == 'È∫µÂÆ∂‰∏âÂ£´' \\\n",
    "        or item[0][:4] == 'ÊãâÈ∫µ‰∫åÈÉé' or item[0][:3] == '‰∏ÄÈ¢®Â†Ç' or item[0][:4] == 'È≥•‰∫∫ÊãâÈ∫µ' or item[0][:4] == 'Â§™ÈôΩ' \\\n",
    "        or item[0][:4] == 'Á•ûÂ±±ÊãâÈ∫µ'or item[0][:4] == 'Ëµ§ÂùÇÊãâÈ∫µ' or item[0][:4] == 'ÂäõÈáèÊãâÈ∫µ' or item[0][:4]== '‰∫¨Ê≠£ÊãâÈ∫µ':\n",
    "        fourth_sort.append([item])   \n",
    "    else:\n",
    "        if len(item) > 1 and len(item[0]) >= 4:\n",
    "            temp = sorted(item, key = util_func)\n",
    "            fourth_sort.append([list(ele) for i, ele in groupby(temp, util_func)])        \n",
    "        elif len(item) > 1 and len(item[0]) <= 3:\n",
    "            fourth_sort.append([item])\n",
    "        else:\n",
    "            unique.append(item[0])\n",
    "\n",
    "fourth_sort_len = []\n",
    "\n",
    "for item_1 in fourth_sort:\n",
    "    for item_2 in item_1: \n",
    "        if len(item_2) > 1:\n",
    "            item_2 = sorted(item_2, key=len)\n",
    "            fourth_sort_len.append(item_2)\n",
    "        else:\n",
    "            unique.append(item_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = []\n",
    "unclassified_group = []\n",
    "\n",
    "for item in fourth_sort_len:\n",
    "    grouped = []\n",
    "    unclassified = []\n",
    "    grouped.append(item[0])\n",
    "    for i in range(len(item)-1):\n",
    "        if (item[0] in item[i+1]) or (item[0][:6] in item[i+1]):\n",
    "            grouped.append(item[i+1])\n",
    "        else:\n",
    "            unclassified.append(item[i+1])\n",
    "    done.append(grouped)\n",
    "    if unclassified != []:\n",
    "        unclassified_group.append(unclassified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FINAL GROUPING\n",
    "for item in unclassified_group:\n",
    "    if len(item) != 1:\n",
    "        temp_grouped = []\n",
    "        temp_unclassified = []\n",
    "        temp_grouped.append(item[0])\n",
    "        for i in range(len(item)-1):\n",
    "            if (item[0] in item[i+1]) or (item[0][:len(item[0])//2] in item[i+1]):\n",
    "                temp_grouped.append(item[i+1])\n",
    "            else:\n",
    "                temp_unclassified.append(item[i+1])\n",
    "        done.append(temp_grouped)\n",
    "        #print(temp_unclassified)\n",
    "        if temp_unclassified != [] and len(temp_unclassified) == 1:\n",
    "            unique.append(temp_unclassified[0])\n",
    "    else:\n",
    "        unique.append(item[0])\n",
    "#unique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_done = {}\n",
    "for i in range(len(done)):\n",
    "    dict_done[i] = done[i]\n",
    "#dict_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ = []\n",
    "\n",
    "for u in unique:\n",
    "    unique_.append([u])\n",
    "dict_unique = {i: unique_[j] for i,j in zip([ i for i in range(len(dict_done), len(unique)+len(dict_done))], [j for j in range(len(unique))])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_fb = dict(dict_done)\n",
    "dict_fb.update(dict_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAPPING SAME NAME STORE WITH MAP\n",
    "store_id = []\n",
    "\n",
    "for store in list(df_new['stem_store']):\n",
    "    for key, v in dict_fb.items():\n",
    "        if store in v:\n",
    "            store_id.append(key)\n",
    "            break\n",
    "        elif (key == list(dict_fb.keys())[-1]):\n",
    "            store_id.append('9999')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['store_id'] = store_id\n",
    "df_new = df_new[['post_id','store_id','stores', 'stem_store','create_on', 'ramen_name', 'fb_review']]\n",
    "df_new.to_csv('store_id_done.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load map.csv\n",
    "df_table_1 = read_csv('Main_Store.csv', encoding='utf8',index_col=0)\n",
    "map_id = list(df_table_1.index)\n",
    "map_name = list(df_table_1['main_store'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_match_id = []\n",
    "\n",
    "for fb_key, fb_v in dict_fb.items():\n",
    "    fb_v[0] = fb_v[0].lower()\n",
    "    fb_v[0] = fb_v[0].replace('„ÅÆ','‰πã')\n",
    "    for m_id, m_name in zip(map_id, map_name):\n",
    "        m_name = m_name.lower()\n",
    "        m_name = m_name.replace('„ÅÆ','‰πã')\n",
    "        if len(fb_v) == 1 and (fb_v[0] == m_name):\n",
    "            fb_match_id.append(m_id)\n",
    "            break\n",
    "        elif (fb_v[0] == m_name):\n",
    "            fb_match_id.append(m_id)\n",
    "            break\n",
    "        elif (len(fb_v) >= 1) and (fb_v[0] in m_name):\n",
    "            fb_match_id.append(m_id)\n",
    "            break\n",
    "        elif (len(fb_v[0]) >= 6) and (fb_v[0][:5] == m_name[:5]):\n",
    "            fb_match_id.append(m_id)\n",
    "            #print(m_name[:6])\n",
    "            break\n",
    "        elif (len(fb_v[0]) >= 5) and (len(fb_v[0]) <= 7) and (fb_v[0][:4] == m_name[:4]):\n",
    "            fb_match_id.append(m_id)\n",
    "            #print(m_name)\n",
    "            break       \n",
    "        elif (m_id == 335):\n",
    "            fb_match_id.append('99999')\n",
    "            break           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATE STORE-ID\n",
    "'''\n",
    "IF the store exsits in Table_1 (Main_Store), store_id matches together;\n",
    "Otherwise, the store_id is 10000+number, which means this store only exists in FB's Post.\n",
    "'''\n",
    "table_foreign_key = []\n",
    "\n",
    "for store in list(df_new['stem_store']):\n",
    "    for (fb_k, fb_v), new in zip(dict_fb.items(), fb_match_id):\n",
    "        if store in fb_v and (new != '99999'):\n",
    "            table_foreign_key.append(new)\n",
    "            break\n",
    "        elif (store in fb_v) and (new == '99999'):\n",
    "            table_foreign_key.append(fb_k + 10000)\n",
    "            break\n",
    "        elif (fb_k == list(dict_fb.keys())[-1]) and (store not in fb_v):\n",
    "            table_foreign_key.append('99999')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RE-Create post_id \n",
    "post_id = ['t3'+ str(i) for i in range(len(table_foreign_key ))]\n",
    "random.shuffle(post_id)\n",
    "df_new['post_id'] = post_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['store_id'] = table_foreign_key \n",
    "df_t3 = df_new[['post_id','store_id','stores', 'create_on', 'ramen_name', 'fb_review']]\n",
    "df_t3.to_csv('Post.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create post_id \n",
    "import pandas as pd\n",
    "post_id = ['T3'+ str(i) for i in range(1,1090)]\n",
    "a = pd.DataFrame({'id':post_id})\n",
    "a.to_csv('t2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "datetime_str = '2020Âπ¥2Êúà20Êó•'\n",
    "datetime_str = datetime_str [2:]\n",
    "print(datetime_str)\n",
    "datetime_str = datetime_str.replace('Âπ¥','/').replace('Êúà','/').replace('Êó•','')\n",
    "print(datetime_str)\n",
    "# Convert String ( ‚ÄòDD/MM/YY HH:MM:SS ‚Äò) to datetime object\n",
    "datetime_obj = datetime.strptime(datetime_str, '%y/%m/%d')\n",
    "print(datetime_obj.date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "\n",
    "df = read_csv('/Users/yuchiacheng/Desktop/FB_Crawling_and_shopname_mapping_with_Map/Tables/Store.csv', encoding='utf8',index_col=0)\n",
    "'''time = df['create_on']\n",
    "\n",
    "\n",
    "data = []\n",
    "\n",
    "for t in time:\n",
    "    tt = t[2:]\n",
    "    tt = tt.replace('Âπ¥','/').replace('Êúà','/').replace('Êó•','')  \n",
    "    t_obj = datetime.strptime(tt, '%y/%m/%d')\n",
    "    data.append(t_obj.date())\n",
    "    #print(t_obj.date())\n",
    "data\n",
    "df['create_on'] = data'''\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "post_id = ['222' + str(i) for i in range(1,461)]\n",
    "#a = pd.DataFrame({'id':post_id})\n",
    "#a.to_csv('t2.csv')\n",
    "df['detail_store_id'] = post_id\n",
    "df\n",
    "df.to_csv('Store_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "datetime_str = '2020Âπ¥2Êúà20Êó•'\n",
    "datetime_str = datetime_str [2:]\n",
    "print(datetime_str)\n",
    "datetime_str = datetime_str.replace('Âπ¥','/').replace('Êúà','/').replace('Êó•','')\n",
    "print(datetime_str)\n",
    "# Convert String ( ‚ÄòDD/MM/YY HH:MM:SS ‚Äò) to datetime object\n",
    "datetime_obj = datetime.strptime(datetime_str, '%y/%m/%d')\n",
    "print(datetime_obj.date())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "post_id = []\n",
    "for i in range(1,1090):\n",
    "    if len(str(i)) == 1:\n",
    "        id_ = 'T3' + '0000' + str(i)\n",
    "    elif len(str(i)) == 2:\n",
    "        id_ = 'T3' + '000' + str(i)\n",
    "    elif len(str(i)) == 3:\n",
    "        id_ = 'T3' + '00' + str(i)\n",
    "    elif len(str(i)) == 4:\n",
    "        id_ = 'T3' + '0' + str(i)\n",
    "    elif len(str(i)) == 6:\n",
    "        id_ = 'T3' + str(i)\n",
    "    post_id.append(id_)\n",
    "    \n",
    "        \n",
    "\n",
    "#\n",
    "\n",
    "#df['detail_store_id'] = post_id\n",
    "#df\n",
    "#df.to_csv('Store_new.csv')\n",
    "post_id\n",
    "a = pd.DataFrame({'id':post_id})\n",
    "a.to_csv('t2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python37364bitbasecondaa34aaf43f98b497db7a5252c786d5680"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
